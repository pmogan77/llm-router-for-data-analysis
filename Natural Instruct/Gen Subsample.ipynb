{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed78cc-839e-4b90-8b35-68f6454a2f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T07:04:05.784571Z",
     "iopub.status.busy": "2025-11-03T07:04:05.784359Z",
     "iopub.status.idle": "2025-11-03T07:07:01.729037Z",
     "shell.execute_reply": "2025-11-03T07:07:01.728508Z",
     "shell.execute_reply.started": "2025-11-03T07:04:05.784553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset lazily...\n",
      "Total records loaded: 3,082,094\n",
      "Sampling ~132 rows per task from 757 unique task_names\n",
      "Final sampled size: 98,417 rows\n",
      "Saved sampled dataset to natural_instructions_sample_balanced.parquet\n",
      "                                           task_name          task_family\n",
      "0            task073_commonsenseqa_answer_generation   Question Answering\n",
      "1                    task846_pubmedqa_classification  Answer Verification\n",
      "2              task096_conala_list_index_subtraction    Program Execution\n",
      "3    task165_mcscript_question_answering_commonsense   Question Answering\n",
      "4                      task177_para-nmt_paraphrasing         Paraphrasing\n",
      "5                    task560_alt_translation_en_entk          Translation\n",
      "6             task1603_smcalflow_sentence_generation  Dialogue Generation\n",
      "7  task047_miscellaneous_answering_science_questions   Question Answering\n",
      "8                  task211_logic2text_classification         Text to Code\n",
      "9          task862_asdiv_multidiv_question_answering   Question Answering\n"
     ]
    }
   ],
   "source": [
    "!pip install dask[complete] pandas pyarrow --quiet\n",
    "\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "DATASET_PATH = \"hf://datasets/Muennighoff/natural-instructions/**/train/*.jsonl\"\n",
    "TASK_MAP_FILE = \"task_to_category_map.json\"  \n",
    "OUTPUT_PATH = \"natural_instructions_sample_balanced.parquet\"\n",
    "TARGET_SIZE = 100_000\n",
    "SEED = 42\n",
    "REPARTITIONS = 200\n",
    "\n",
    "with open(TASK_MAP_FILE, \"r\") as f:\n",
    "    task_name_to_family = json.load(f)\n",
    "\n",
    "print(\"Reading dataset lazily...\")\n",
    "bag = db.read_text(DATASET_PATH)\n",
    "\n",
    "def safe_parse(line):\n",
    "    try:\n",
    "        ex = json.loads(line)\n",
    "        task_name = ex.get(\"task_name\", \"\")\n",
    "        return {\n",
    "            \"task_name\": task_name,\n",
    "            \"task_family\": task_name_to_family.get(task_name, \"other\"),\n",
    "            \"id\": ex.get(\"id\", \"\"),\n",
    "            \"definition\": str(ex.get(\"definition\", \"\")),\n",
    "            \"inputs\": str(ex.get(\"inputs\", \"\")),\n",
    "            \"targets\": str(ex.get(\"targets\", \"\")),\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "parsed = bag.map(safe_parse).filter(lambda x: x is not None)\n",
    "\n",
    "df = parsed.to_dataframe()\n",
    "\n",
    "df = df.repartition(npartitions=REPARTITIONS)\n",
    "\n",
    "cols = [\"task_name\", \"task_family\", \"id\", \"definition\", \"inputs\", \"targets\"]\n",
    "df_pd = df[cols].compute()\n",
    "print(f\"Total records loaded: {len(df_pd):,}\")\n",
    "\n",
    "task_names = df_pd[\"task_name\"].dropna().unique()\n",
    "rows_per_task = max(1, TARGET_SIZE // len(task_names))\n",
    "print(f\"Sampling ~{rows_per_task} rows per task from {len(task_names)} unique task_names\")\n",
    "\n",
    "sampled_list = []\n",
    "\n",
    "for task in task_names:\n",
    "    task_df = df_pd[df_pd[\"task_name\"] == task]\n",
    "    n = min(len(task_df), rows_per_task)  # avoid sampling more than available\n",
    "    if n == 0:\n",
    "        continue  # task_name exists but has no rows (just in case)\n",
    "    sampled_list.append(task_df.sample(n=n, random_state=SEED))\n",
    "\n",
    "# Concatenate all samples\n",
    "sampled_df = pd.concat(sampled_list, ignore_index=True)\n",
    "\n",
    "# Shuffle final sample\n",
    "sampled_df = sampled_df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "print(f\"Final sampled size: {len(sampled_df):,} rows\")\n",
    "\n",
    "sampled_df.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved sampled dataset to {OUTPUT_PATH}\")\n",
    "\n",
    "# Preview\n",
    "print(sampled_df[[\"task_name\", \"task_family\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828712b8-dee9-4945-ab71-1be70b0b4cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
