{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97e64d-a0b6-49c1-9678-a9896296bec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:30:38.171456Z",
     "iopub.status.busy": "2025-12-02T08:30:38.171237Z",
     "iopub.status.idle": "2025-12-02T08:39:43.961778Z",
     "shell.execute_reply": "2025-12-02T08:39:43.961083Z",
     "shell.execute_reply.started": "2025-12-02T08:30:38.171439Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers rouge nltk\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "empty_cases = {\"both_empty\": 0, \"only_one_empty\": 0}\n",
    "\n",
    "def exact_match_accuracy(pred: str, gold: str) -> float:\n",
    "    p, g = str(pred).strip(), str(gold).strip()\n",
    "    if not p and not g:\n",
    "        empty_cases[\"both_empty\"] += 1\n",
    "        return 1.0\n",
    "    if not p or not g:\n",
    "        empty_cases[\"only_one_empty\"] += 1\n",
    "        return 0.0\n",
    "    return 1.0 if p == g else 0.0\n",
    "\n",
    "def normalized_cosine_similarity(pred: str, gold: str, model: SentenceTransformer) -> float:\n",
    "    p, g = str(pred).strip(), str(gold).strip()\n",
    "    if not p and not g:\n",
    "        empty_cases[\"both_empty\"] += 1\n",
    "        return 1.0\n",
    "    if not p or not g:\n",
    "        empty_cases[\"only_one_empty\"] += 1\n",
    "        return 0.0\n",
    "    emb_pred = model.encode(p, convert_to_tensor=True)\n",
    "    emb_gold = model.encode(g, convert_to_tensor=True)\n",
    "    cosine_score = util.cos_sim(emb_pred, emb_gold).item()\n",
    "    return (cosine_score + 1) / 2\n",
    "\n",
    "def bleu_score(pred: str, gold: str) -> float:\n",
    "    p, g = str(pred).strip(), str(gold).strip()\n",
    "    if not p and not g:\n",
    "        empty_cases[\"both_empty\"] += 1\n",
    "        return 1.0\n",
    "    if not p or not g:\n",
    "        empty_cases[\"only_one_empty\"] += 1\n",
    "        return 0.0\n",
    "    ref_tokens = g.split()\n",
    "    pred_tokens = p.split()\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "\n",
    "def rouge_l_score(pred: str, gold: str) -> float:\n",
    "    p, g = str(pred).strip(), str(gold).strip()\n",
    "    if not p and not g:\n",
    "        empty_cases[\"both_empty\"] += 1\n",
    "        return 1.0\n",
    "    if not p or not g:\n",
    "        empty_cases[\"only_one_empty\"] += 1\n",
    "        return 0.0\n",
    "    rouge = Rouge()\n",
    "    try:\n",
    "        scores = rouge.get_scores(p, g)\n",
    "        return scores[0]['rouge-l']['f']\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# Task Category Groups\n",
    "CLASSIFICATION_CATEGORIES = [\n",
    "    'Answer Verification', 'Answerability Classification', 'Cause Effect Classification',\n",
    "    'Coherence Classification', 'Commonsense Classification', 'Coreference Resolution',\n",
    "    'Dialogue Act Recognition', 'Discourse Connective Identification',\n",
    "    'Discourse Relation Classification', 'Ethics Classification', 'Gender Classification',\n",
    "    'Irony Detection', 'Intent Identification', 'Named Entity Recognition',\n",
    "    'Negotiation Strategy Detection', 'Overlap Extraction', 'Pos Tagging',\n",
    "    'Preposition Prediction', 'Section Classification', 'Spam Classification',\n",
    "    'Speaker Identification', 'Speaker Relation Classification', 'Stance Detection',\n",
    "    'Stereotype Detection', 'Toxic Language Detection', 'Word Relation Classification',\n",
    "    'Fill in The Blank', 'Language Identification', 'Keyword Tagging',\n",
    "    'Grammar Error Correction', 'Grammar Error Detection', 'Punctuation Error Detection',\n",
    "    'Spelling Error Detection', 'Misc.', 'Dialogue State Tracking',\n",
    "    'Information Extraction', 'Sentiment Analysis', 'Entity Relation Classification',\n",
    "    'Textual Entailment', 'Text Categorization', 'Linguistic Probing'\n",
    "]\n",
    "\n",
    "SBERT_CATEGORIES = [\n",
    "    'Sentence Composition', 'Sentence Compression', 'Sentence Expansion',\n",
    "    'Sentence Ordering', 'Sentence Perturbation', 'Story Composition',\n",
    "    'Style Transfer', 'Entity Generation', 'Text Matching', 'Text Quality Evaluation',\n",
    "    'Question Answering', 'Wrong Candidate Generation'\n",
    "]\n",
    "\n",
    "BLEU_CATEGORIES = [\n",
    "    'Paraphrasing', 'Text Simplification', 'Text to Code',\n",
    "    'Title Generation', 'Translation', 'Question Rewriting', 'Question Generation',\n",
    "    'Question Decomposition'\n",
    "]\n",
    "\n",
    "ROUGE_CATEGORIES = [\n",
    "    'Summarization', 'Code to Text', 'Data to Text', 'Dialogue Generation',\n",
    "    'Explanation', 'Paper Review', 'Text Completion', 'Poem Generation',\n",
    "]\n",
    "\n",
    "FACT_NUMERIC_CATEGORIES = [\n",
    "    'Fact Verification', \n",
    "    'Question Understanding', 'Word Analogy', 'Word Semantics', 'Mathematics',\n",
    "    'Number Conversion', 'Program Execution'\n",
    "]\n",
    "\n",
    "# Compute best score among predictions and golds\n",
    "def best_score(preds, golds, category, model):\n",
    "    best = 0.0\n",
    "    for p in preds:\n",
    "        for g in golds:\n",
    "            if category in CLASSIFICATION_CATEGORIES or category in FACT_NUMERIC_CATEGORIES:\n",
    "                score = exact_match_accuracy(p, g)\n",
    "            elif category in SBERT_CATEGORIES:\n",
    "                score = normalized_cosine_similarity(p, g, model)\n",
    "            elif category in BLEU_CATEGORIES:\n",
    "                score = bleu_score(p, g)\n",
    "            elif category in ROUGE_CATEGORIES:\n",
    "                score = rouge_l_score(p, g)\n",
    "            else:\n",
    "                score = exact_match_accuracy(p, g)\n",
    "            if score > best:\n",
    "                best = score\n",
    "    return best\n",
    "\n",
    "# Compute metrics per task_name/input group\n",
    "def compute_metrics_for_task(df: pd.DataFrame, task_name: str, category: str, model: SentenceTransformer) -> dict:\n",
    "    task_df = df[df['task_name'] == task_name]\n",
    "    if len(task_df) == 0:\n",
    "        return {'avg_acc': np.nan, 'avg_latency': np.nan, 'num_score_records': 0, 'num_latency_records': 0}\n",
    "\n",
    "    scores, latencies = [], []\n",
    "\n",
    "    grouped = task_df.groupby(['task_name', 'inputs'])\n",
    "    for (tname, input_val), group in grouped:\n",
    "        preds = group['output_text'].tolist()\n",
    "        golds = group['targets'].tolist()\n",
    "        scores.append(best_score(preds, golds, category, model))\n",
    "        latencies.extend(group['latency_sec'].tolist())\n",
    "\n",
    "    # Get representative definition (first one)\n",
    "    definition = task_df['definition'].iloc[0] if 'definition' in task_df.columns else \"\"\n",
    "\n",
    "    return {\n",
    "        'avg_acc': float(np.mean(scores)),\n",
    "        'avg_latency': float(np.mean(latencies)),\n",
    "        'num_score_records': len(grouped),\n",
    "        'num_latency_records': len(latencies),\n",
    "        'definition': definition\n",
    "    }\n",
    "\n",
    "def main(results_folder: str, task_map_file: str, task_domain_map_file: str, task_summary_file: str, output_file: str, model_name: str):\n",
    "    # Load parquet files\n",
    "    all_files = glob.glob(f\"{results_folder}/*.parquet\")\n",
    "    df_list = [pd.read_parquet(f) for f in all_files]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Loaded {len(df)} records from {len(all_files)} parquet files.\")\n",
    "\n",
    "    # Load task_name -> category map\n",
    "    with open(task_map_file, 'r') as f:\n",
    "        task_to_category = json.load(f)\n",
    "\n",
    "    # Load task_name -> domain map\n",
    "    with open(task_domain_map_file, 'r') as f:\n",
    "        task_to_domain = json.load(f)\n",
    "\n",
    "    # Load summaries (short definitions)\n",
    "    with open(task_summary_file, 'r') as f:\n",
    "        summary_data = json.load(f)\n",
    "    task_to_summary = {\n",
    "        item[\"Name\"].strip(\"`\"): item[\"Summary\"]\n",
    "        for item in summary_data if \"Name\" in item and \"Summary\" in item\n",
    "    }\n",
    "\n",
    "    # Load SBERT model\n",
    "    sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "    # Compute metrics per task_name\n",
    "    results = []\n",
    "    for task_name, category in task_to_category.items():\n",
    "        task_df = df[df['task_name'] == task_name]\n",
    "        num_records = len(task_df)\n",
    "        if num_records == 0:\n",
    "            print(f\"No records found for task: {task_name} (category={category})\")\n",
    "            continue\n",
    "\n",
    "        metrics = compute_metrics_for_task(df, task_name, category, sbert_model)\n",
    "\n",
    "        results.append({\n",
    "            'task_name': task_name,\n",
    "            'avg_acc': metrics['avg_acc'],\n",
    "            'avg_latency': metrics['avg_latency'],\n",
    "            'num_score_records': metrics['num_score_records'],\n",
    "            'num_latency_records': metrics['num_latency_records'],\n",
    "            'model_name': model_name,\n",
    "            'category': category,\n",
    "            'domain': task_to_domain.get(task_name, \"Unknown\"),\n",
    "            'definition': metrics['definition'],\n",
    "            'short_definition': task_to_summary.get(task_name, \"\")\n",
    "        })\n",
    "\n",
    "    # Save to CSV\n",
    "    pd.DataFrame(results)[\n",
    "        ['task_name', 'avg_acc', 'avg_latency', 'num_score_records', 'num_latency_records',\n",
    "         'model_name', 'category', 'domain', 'definition', 'short_definition']\n",
    "    ].to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MODEL_NAME = \"microsoft_Phi-4-mini-instruct\"\n",
    "\n",
    "    # Create unique run folder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RUN_FOLDER = f\"runs/{MODEL_NAME}_{timestamp}\"\n",
    "    os.makedirs(RUN_FOLDER, exist_ok=True)\n",
    "\n",
    "    # Redirect ALL outputs under this folder\n",
    "    RESULTS_FOLDER = \"results_chunks_\" + MODEL_NAME\n",
    "    os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "\n",
    "    TASK_MAP_FILE = \"task_to_category_map.json\"\n",
    "    TASK_DOMAIN_MAP_FILE = \"task_to_domain_map.json\"\n",
    "    TASK_SUMMARY_FILE = \"task_summary.json\"\n",
    "\n",
    "    OUTPUT_FILE = os.path.join(RUN_FOLDER, \"task_metrics_\" + MODEL_NAME + \".csv\")\n",
    "\n",
    "    # Run main\n",
    "    main(RESULTS_FOLDER, TASK_MAP_FILE, TASK_DOMAIN_MAP_FILE, TASK_SUMMARY_FILE, OUTPUT_FILE, MODEL_NAME)\n",
    "\n",
    "    # Save empty stats as a text file inside run folder\n",
    "    with open(os.path.join(RUN_FOLDER, \"empty_case_stats.txt\"), \"w\") as f:\n",
    "        f.write(f\"Both empty (correct): {empty_cases['both_empty']}\\n\")\n",
    "        f.write(f\"One empty (incorrect): {empty_cases['only_one_empty']}\\n\")\n",
    "\n",
    "    print(\"\\nEmpty case stats:\")\n",
    "    print(f\" Both empty (counted as correct): {empty_cases['both_empty']}\")\n",
    "    print(f\" One empty (counted as incorrect): {empty_cases['only_one_empty']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
